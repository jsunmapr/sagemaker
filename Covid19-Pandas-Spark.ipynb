{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Covid-19 confirmed cases csv from Johns Hopkins Github, clean it, and use Pandas to visualize. Also save to S3 in Parquet format and created Hive external tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO # python3; python2: BytesIO \n",
    "\n",
    "data_url ='https://bit.ly/3d93pa1' #Download confirmed cases from Johns Hopkins Github repo\n",
    "pdf = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose and extract selected columns(countries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1 = pdf.T.drop(['Lat','Long','Province/State'])\n",
    "new_header = pdf1.iloc[0]\n",
    "pdf2 = pdf1[1:] \n",
    "pdf2.columns = new_header\n",
    "pdf3 = pdf2[['Brazil', 'India', 'Germany', 'Italy', 'Spain', 'US']]\n",
    "pdf3['date'] = pd.date_range(start='1/22/2020', periods=len(pdf3), freq='D')\n",
    "pdf3.columns = ['Brazil', 'India', 'Germany', 'Italy', 'Spain', 'US', 'date']\n",
    "pdf3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the case increase over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# a scatter plot comparing num_children and num_pets\n",
    "pdf3.plot('date',['US', 'Brazil', 'Italy', 'India', 'Spain', 'Germany'])\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "labels = 'Brazil','India','Germany','Italy','Spain','US'\n",
    "confirmed = pdf3.tail(1).iloc[0,0:6]\n",
    "explode = (0, 0, 0, 0, 0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(confirmed, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Pandas dataframe to S3 as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'mybucket' # already created on S3\n",
    "csv_buffer = StringIO()\n",
    "pdf3.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'pandas/cv.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Pandas dataframe (pdf3) to Spark dataframe (sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "sdf = spark.createDataFrame(pdf3)\n",
    "sdf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Spark dataframe to S3 in Parquet format, this way the schema is preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.write.parquet(\"s3://mybucket/parquet/\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Spark dataframe (sdf) to hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView(\"mytempTable\") \n",
    "sqlContext.sql(\"drop table if exists CV19\")\n",
    "sqlContext.sql(\"create table if not exists CV19 as select * from mytempTable\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from CV19\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or simply just use %%sql to run the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from CV19 where US >= 6000000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
